"""
–î–µ–º–æ–Ω—Å—Ç—Ä–∞—Ü–∏—è —Ä–∞–±–æ—Ç—ã SARIF –ø–∞—Ä—Å–µ—Ä–∞
–í–∏–∑—É–∞–ª—å–Ω–∞—è –ø—Ä–µ–∑–µ–Ω—Ç–∞—Ü–∏—è –≤–æ–∑–º–æ–∂–Ω–æ—Å—Ç–µ–π
"""

from sarif_parser import SarifParser, SeverityLevel, print_report_summary
import os


def print_header(text):
    """–ö—Ä–∞—Å–∏–≤—ã–π –∑–∞–≥–æ–ª–æ–≤–æ–∫"""
    print(f"\n{'='*80}")
    print(f"  {text}")
    print(f"{'='*80}\n")


def print_section(title):
    """–ó–∞–≥–æ–ª–æ–≤–æ–∫ —Å–µ–∫—Ü–∏–∏"""
    print(f"\n{'-'*80}")
    print(f"üìå {title}")
    print(f"{'-'*80}")


def demo_basic_parsing():
    """–î–µ–º–æ 1: –ë–∞–∑–æ–≤—ã–π –ø–∞—Ä—Å–∏–Ω–≥"""
    print_header("–î–ï–ú–û 1: –ë–∞–∑–æ–≤—ã–π –ø–∞—Ä—Å–∏–Ω–≥ SARIF —Ñ–∞–π–ª–∞")
    
    file_path = r"C:\Users\ssinyakov\Downloads\bbs2_ru.sarif"
    
    if not os.path.exists(file_path):
        print(f"‚ö†Ô∏è  –§–∞–π–ª –Ω–µ –Ω–∞–π–¥–µ–Ω: {file_path}")
        print("–ò—Å–ø–æ–ª—å–∑—É–π—Ç–µ —Å–≤–æ–π –ø—É—Ç—å –∫ SARIF —Ñ–∞–π–ª—É")
        return
    
    print(f"üìÇ –§–∞–π–ª: {os.path.basename(file_path)}")
    print(f"üìä –†–∞–∑–º–µ—Ä: {os.path.getsize(file_path) / 1024:.1f} KB\n")
    
    # –ü–∞—Ä—Å–∏–Ω–≥
    print("‚è≥ –ü–∞—Ä—Å–∏–Ω–≥...")
    report = SarifParser.parse_file(file_path)
    print("‚úÖ –ì–æ—Ç–æ–≤–æ!\n")
    
    # –ë–∞–∑–æ–≤–∞—è –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—è
    print(f"üîß –ò–Ω—Å—Ç—Ä—É–º–µ–Ω—Ç: {report.tool.name}")
    if report.tool.version:
        print(f"üì¶ –í–µ—Ä—Å–∏—è: {report.tool.version}")
    if report.tool.organization:
        print(f"üè¢ –û—Ä–≥–∞–Ω–∏–∑–∞—Ü–∏—è: {report.tool.organization}")
    
    print(f"\nüìã –í—Å–µ–≥–æ –ø—Ä–∞–≤–∏–ª: {len(report.rules)}")
    print(f"üîç –í—Å–µ–≥–æ –Ω–∞—Ö–æ–¥–æ–∫: {len(report.findings)}")


def demo_statistics():
    """–î–µ–º–æ 2: –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞"""
    print_header("–î–ï–ú–û 2: –î–µ—Ç–∞–ª—å–Ω–∞—è —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞")
    
    file_path = r"C:\Users\ssinyakov\Downloads\bbs1_ru.sarif"
    
    if not os.path.exists(file_path):
        print(f"‚ö†Ô∏è  –§–∞–π–ª –Ω–µ –Ω–∞–π–¥–µ–Ω: {file_path}")
        return
    
    report = SarifParser.parse_file(file_path)
    stats = report.get_statistics()
    
    print_section("–†–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ –ø–æ —É—Ä–æ–≤–Ω—è–º —Å–µ—Ä—å–µ–∑–Ω–æ—Å—Ç–∏")
    
    total = stats['total_findings']
    
    # –í–∏–∑—É–∞–ª—å–Ω—ã–µ –∏–Ω–¥–∏–∫–∞—Ç–æ—Ä—ã
    error_count = stats['by_severity']['error']
    warning_count = stats['by_severity']['warning']
    note_count = stats['by_severity']['note']
    
    def print_bar(label, count, total, icon, color):
        """–†–∏—Å—É–µ—Ç –ø—Ä–æ–≥—Ä–µ—Å—Å-–±–∞—Ä"""
        percentage = (count / total * 100) if total > 0 else 0
        bar_length = 40
        filled = int(bar_length * count / total) if total > 0 else 0
        bar = '‚ñà' * filled + '‚ñë' * (bar_length - filled)
        print(f"{icon} {label:15} [{bar}] {count:4} ({percentage:5.1f}%)")
    
    print_bar("–û—à–∏–±–∫–∏", error_count, total, "üî¥", "red")
    print_bar("–ü—Ä–µ–¥—É–ø—Ä–µ–∂–¥–µ–Ω–∏—è", warning_count, total, "üü°", "yellow")
    print_bar("–ó–∞–º–µ—Ç–∫–∏", note_count, total, "üîµ", "blue")
    
    print_section("–¢–æ–ø-10 —Ç–∏–ø–æ–≤ —É—è–∑–≤–∏–º–æ—Å—Ç–µ–π")
    
    sorted_rules = sorted(
        stats['by_rule'].items(),
        key=lambda x: x[1]['count'],
        reverse=True
    )
    
    for i, (rule_id, info) in enumerate(sorted_rules[:10], 1):
        icon = {
            'error': 'üî¥',
            'warning': 'üü°',
            'note': 'üîµ',
            'none': '‚ö™'
        }.get(info['level'], '‚ö™')
        
        print(f"{i:2}. {icon} {info['name']}")
        print(f"    ‚îî‚îÄ –ù–∞—Ö–æ–¥–æ–∫: {info['count']} | ID: {rule_id}")


def demo_filtering():
    """–î–µ–º–æ 3: –§–∏–ª—å—Ç—Ä–∞—Ü–∏—è"""
    print_header("–î–ï–ú–û 3: –§–∏–ª—å—Ç—Ä–∞—Ü–∏—è –Ω–∞—Ö–æ–¥–æ–∫")
    
    file_path = r"C:\Users\ssinyakov\Downloads\AI3.sarif"
    
    if not os.path.exists(file_path):
        print(f"‚ö†Ô∏è  –§–∞–π–ª –Ω–µ –Ω–∞–π–¥–µ–Ω: {file_path}")
        return
    
    report = SarifParser.parse_file(file_path)
    
    print_section("–§–∏–ª—å—Ç—Ä 1: –¢–æ–ª—å–∫–æ –∫—Ä–∏—Ç–∏—á–µ—Å–∫–∏–µ —É—è–∑–≤–∏–º–æ—Å—Ç–∏")
    
    critical = [f for f in report.findings if f.level == SeverityLevel.ERROR]
    print(f"–ù–∞–π–¥–µ–Ω–æ –∫—Ä–∏—Ç–∏—á–µ—Å–∫–∏—Ö: {len(critical)}")
    
    if critical:
        print("\n–ü—Ä–∏–º–µ—Ä—ã:")
        for i, finding in enumerate(critical[:3], 1):
            print(f"\n{i}. {finding.rule_name}")
            print(f"   ID: {finding.rule_id}")
            if finding.locations:
                print(f"   –§–∞–π–ª: {finding.locations[0].file_path}")
                if finding.locations[0].start_line:
                    print(f"   –°—Ç—Ä–æ–∫–∞: {finding.locations[0].start_line}")
    
    print_section("–§–∏–ª—å—Ç—Ä 2: –ü–æ —Ç–∏–ø—É —É—è–∑–≤–∏–º–æ—Å—Ç–∏")
    
    # –ò—â–µ–º SQL Injection
    sqli = [f for f in report.findings if 'sql' in f.rule_id.lower()]
    print(f"SQL Injection: {len(sqli)}")
    
    # –ò—â–µ–º XSS
    xss = [f for f in report.findings if 'xss' in f.rule_id.lower()]
    print(f"Cross-Site Scripting (XSS): {len(xss)}")
    
    # –ò—â–µ–º RCE
    rce = [f for f in report.findings if any(x in f.rule_id.lower() for x in ['rce', 'code execution'])]
    print(f"Remote Code Execution (RCE): {len(rce)}")


def demo_file_analysis():
    """–î–µ–º–æ 4: –ê–Ω–∞–ª–∏–∑ —Ñ–∞–π–ª–æ–≤"""
    print_header("–î–ï–ú–û 4: –ê–Ω–∞–ª–∏–∑ –ø–æ—Ä–∞–∂–µ–Ω–Ω—ã—Ö —Ñ–∞–π–ª–æ–≤")
    
    file_path = r"C:\Users\ssinyakov\Downloads\bbs1_ru.sarif"
    
    if not os.path.exists(file_path):
        print(f"‚ö†Ô∏è  –§–∞–π–ª –Ω–µ –Ω–∞–π–¥–µ–Ω: {file_path}")
        return
    
    report = SarifParser.parse_file(file_path)
    
    print_section("–ê–Ω–∞–ª–∏–∑ –ø–æ —Ñ–∞–π–ª–∞–º")
    
    # –°–æ–±–∏—Ä–∞–µ–º —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫—É –ø–æ —Ñ–∞–π–ª–∞–º
    from collections import defaultdict
    
    files_stats = defaultdict(lambda: {
        'total': 0,
        'critical': 0,
        'warnings': 0,
        'notes': 0,
        'types': set()
    })
    
    for finding in report.findings:
        for location in finding.locations:
            if location.file_path:
                file = location.file_path
                files_stats[file]['total'] += 1
                files_stats[file]['types'].add(finding.rule_id)
                
                if finding.level == SeverityLevel.ERROR:
                    files_stats[file]['critical'] += 1
                elif finding.level == SeverityLevel.WARNING:
                    files_stats[file]['warnings'] += 1
                else:
                    files_stats[file]['notes'] += 1
    
    print(f"–í—Å–µ–≥–æ —É–Ω–∏–∫–∞–ª—å–Ω—ã—Ö —Ñ–∞–π–ª–æ–≤ —Å –ø—Ä–æ–±–ª–µ–º–∞–º–∏: {len(files_stats)}")
    
    # –¢–æ–ø-10 —Ñ–∞–π–ª–æ–≤
    sorted_files = sorted(
        files_stats.items(),
        key=lambda x: x[1]['total'],
        reverse=True
    )
    
    print("\n–¢–æ–ø-10 —Ñ–∞–π–ª–æ–≤ —Å –Ω–∞–∏–±–æ–ª—å—à–∏–º –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ–º –ø—Ä–æ–±–ª–µ–º:\n")
    
    for i, (file, stats) in enumerate(sorted_files[:10], 1):
        # –£–∫–æ—Ä–∞—á–∏–≤–∞–µ–º –ø—É—Ç—å –¥–ª—è —á–∏—Ç–∞–µ–º–æ—Å—Ç–∏
        short_path = file if len(file) < 60 else "..." + file[-57:]
        
        print(f"{i:2}. {short_path}")
        print(f"    ‚îî‚îÄ –í—Å–µ–≥–æ: {stats['total']} | ", end="")
        print(f"üî¥ {stats['critical']} | ", end="")
        print(f"üü° {stats['warnings']} | ", end="")
        print(f"üîµ {stats['notes']}")
        print(f"       –¢–∏–ø–æ–≤ –ø—Ä–æ–±–ª–µ–º: {len(stats['types'])}")


def demo_export():
    """–î–µ–º–æ 5: –≠–∫—Å–ø–æ—Ä—Ç –¥–∞–Ω–Ω—ã—Ö"""
    print_header("–î–ï–ú–û 5: –≠–∫—Å–ø–æ—Ä—Ç –¥–∞–Ω–Ω—ã—Ö")
    
    file_path = r"C:\Users\ssinyakov\Downloads\bbs2_ru.sarif"
    
    if not os.path.exists(file_path):
        print(f"‚ö†Ô∏è  –§–∞–π–ª –Ω–µ –Ω–∞–π–¥–µ–Ω: {file_path}")
        return
    
    report = SarifParser.parse_file(file_path)
    
    print_section("–≠–∫—Å–ø–æ—Ä—Ç –≤ CSV")
    
    import csv
    
    output_file = "demo_export.csv"
    
    with open(output_file, 'w', newline='', encoding='utf-8') as f:
        writer = csv.DictWriter(f, fieldnames=[
            'Rule ID',
            'Rule Name',
            'Severity',
            'File',
            'Line',
            'Snippet'
        ])
        
        writer.writeheader()
        
        for finding in report.findings:
            for location in finding.locations:
                writer.writerow({
                    'Rule ID': finding.rule_id,
                    'Rule Name': finding.rule_name or '',
                    'Severity': finding.level.value,
                    'File': location.file_path,
                    'Line': location.start_line or '',
                    'Snippet': (location.snippet or '')[:100]
                })
    
    print(f"‚úÖ –≠–∫—Å–ø–æ—Ä—Ç–∏—Ä–æ–≤–∞–Ω–æ –≤: {output_file}")
    print(f"üìä –ó–∞–ø–∏—Å–µ–π: {sum(len(f.locations) for f in report.findings)}")
    
    print_section("–≠–∫—Å–ø–æ—Ä—Ç –≤ JSON")
    
    import json
    
    output_file = "demo_export.json"
    
    data = {
        'metadata': {
            'tool': report.tool.name,
            'version': report.tool.version,
            'total_findings': len(report.findings)
        },
        'statistics': report.get_statistics(),
        'findings': [
            {
                'id': finding.rule_id,
                'name': finding.rule_name,
                'severity': finding.level.value,
                'locations': [
                    {
                        'file': loc.file_path,
                        'line': loc.start_line
                    }
                    for loc in finding.locations
                ]
            }
            for finding in report.findings[:10]  # –¢–æ–ª—å–∫–æ –ø–µ—Ä–≤—ã–µ 10
        ]
    }
    
    with open(output_file, 'w', encoding='utf-8') as f:
        json.dump(data, f, indent=2, ensure_ascii=False)
    
    print(f"‚úÖ –≠–∫—Å–ø–æ—Ä—Ç–∏—Ä–æ–≤–∞–Ω–æ –≤: {output_file}")


def main():
    """–ì–ª–∞–≤–Ω–∞—è —Ñ—É–Ω–∫—Ü–∏—è - –∑–∞–ø—É—Å–∫ –≤—Å–µ—Ö –¥–µ–º–æ"""
    
    print("""
‚ïî‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïó
‚ïë                                                                              ‚ïë
‚ïë                        üîç SARIF PARSER - –î–ï–ú–û–ù–°–¢–†–ê–¶–ò–Ø                        ‚ïë
‚ïë                                                                              ‚ïë
‚ïë             –ü–∞—Ä—Å–µ—Ä –æ—Ç—á–µ—Ç–æ–≤ –æ –±–µ–∑–æ–ø–∞—Å–Ω–æ—Å—Ç–∏ –≤ —Ñ–æ—Ä–º–∞—Ç–µ SARIF 2.1.0             ‚ïë
‚ïë                                                                              ‚ïë
‚ïö‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïù
""")
    
    demos = [
        ("–ë–∞–∑–æ–≤—ã–π –ø–∞—Ä—Å–∏–Ω–≥", demo_basic_parsing),
        ("–î–µ—Ç–∞–ª—å–Ω–∞—è —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞", demo_statistics),
        ("–§–∏–ª—å—Ç—Ä–∞—Ü–∏—è –Ω–∞—Ö–æ–¥–æ–∫", demo_filtering),
        ("–ê–Ω–∞–ª–∏–∑ —Ñ–∞–π–ª–æ–≤", demo_file_analysis),
        ("–≠–∫—Å–ø–æ—Ä—Ç –¥–∞–Ω–Ω—ã—Ö", demo_export),
    ]
    
    print("\n–î–æ—Å—Ç—É–ø–Ω—ã–µ –¥–µ–º–æ–Ω—Å—Ç—Ä–∞—Ü–∏–∏:\n")
    for i, (name, _) in enumerate(demos, 1):
        print(f"  {i}. {name}")
    
    print("\n" + "="*80)
    
    # –ó–∞–ø—É—Å–∫–∞–µ–º –≤—Å–µ –¥–µ–º–æ –ø–æ –æ—á–µ—Ä–µ–¥–∏
    for name, demo_func in demos:
        try:
            demo_func()
            input("\n‚è∏  –ù–∞–∂–º–∏—Ç–µ Enter –¥–ª—è –ø—Ä–æ–¥–æ–ª–∂–µ–Ω–∏—è...")
        except FileNotFoundError as e:
            print(f"\n‚ö†Ô∏è  –ü—Ä–æ–ø—É—â–µ–Ω–æ: —Ñ–∞–π–ª –Ω–µ –Ω–∞–π–¥–µ–Ω")
            input("\n‚è∏  –ù–∞–∂–º–∏—Ç–µ Enter –¥–ª—è –ø—Ä–æ–¥–æ–ª–∂–µ–Ω–∏—è...")
        except Exception as e:
            print(f"\n‚ùå –û—à–∏–±–∫–∞: {e}")
            import traceback
            traceback.print_exc()
            input("\n‚è∏  –ù–∞–∂–º–∏—Ç–µ Enter –¥–ª—è –ø—Ä–æ–¥–æ–ª–∂–µ–Ω–∏—è...")
    
    print_header("–î–ï–ú–û–ù–°–¢–†–ê–¶–ò–Ø –ó–ê–í–ï–†–®–ï–ù–ê")
    print("""
‚ú® –í—Å–µ –≤–æ–∑–º–æ–∂–Ω–æ—Å—Ç–∏ –ø—Ä–æ–¥–µ–º–æ–Ω—Å—Ç—Ä–∏—Ä–æ–≤–∞–Ω—ã!

üìö –î–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω–∞—è –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—è:
  - README_SARIF_PARSER.md  - –ø–æ–ª–Ω–∞—è –¥–æ–∫—É–º–µ–Ω—Ç–∞—Ü–∏—è
  - QUICKSTART.md            - –±—ã—Å—Ç—Ä—ã–π —Å—Ç–∞—Ä—Ç
  - example_usage.py         - –±–æ–ª—å—à–µ –ø—Ä–∏–º–µ—Ä–æ–≤

üöÄ –ì–æ—Ç–æ–≤–æ –∫ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—é –≤ –≤–∞—à–µ–º –ø—Ä–æ–µ–∫—Ç–µ!
""")


if __name__ == "__main__":
    main()

