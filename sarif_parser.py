"""
SARIF Parser - –ø–∞—Ä—Å–µ—Ä –¥–ª—è –æ—Ç—á–µ—Ç–æ–≤ –≤ —Ñ–æ—Ä–º–∞—Ç–µ SARIF 2.1.0

–ü–æ–¥–¥–µ—Ä–∂–∏–≤–∞–µ–º—ã–µ –∏–Ω—Å—Ç—Ä—É–º–µ–Ω—Ç—ã:
- PT BlackBox
- PT Application Inspector
- –î—Ä—É–≥–∏–µ –∏–Ω—Å—Ç—Ä—É–º–µ–Ω—Ç—ã, –∏—Å–ø–æ–ª—å–∑—É—é—â–∏–µ —Å—Ç–∞–Ω–¥–∞—Ä—Ç SARIF 2.1.0
"""

import json
from dataclasses import dataclass, field
from typing import List, Optional, Dict, Any
from pathlib import Path
from enum import Enum


class SeverityLevel(Enum):
    """–£—Ä–æ–≤–Ω–∏ —Å–µ—Ä—å–µ–∑–Ω–æ—Å—Ç–∏ –Ω–∞—Ö–æ–¥–æ–∫"""
    ERROR = "error"
    WARNING = "warning"
    NOTE = "note"
    NONE = "none"


@dataclass
class Rule:
    """–ü—Ä–∞–≤–∏–ª–æ (—Ç–∏–ø —É—è–∑–≤–∏–º–æ—Å—Ç–∏)"""
    id: str
    name: str
    description_text: Optional[str] = None
    description_markdown: Optional[str] = None
    level: SeverityLevel = SeverityLevel.NOTE
    enabled: bool = True
    
    def __repr__(self):
        return f"Rule(id='{self.id}', name='{self.name}', level={self.level.value})"


@dataclass
class Location:
    """–õ–æ–∫–∞—Ü–∏—è –Ω–∞—Ö–æ–¥–∫–∏ –≤ –∫–æ–¥–µ"""
    file_path: str
    snippet: Optional[str] = None
    start_line: Optional[int] = None
    end_line: Optional[int] = None
    start_column: Optional[int] = None
    end_column: Optional[int] = None
    
    def __repr__(self):
        location_str = self.file_path
        if self.start_line:
            location_str += f":{self.start_line}"
            if self.end_line and self.end_line != self.start_line:
                location_str += f"-{self.end_line}"
        return f"Location({location_str})"


@dataclass
class Finding:
    """–ù–∞—Ö–æ–¥–∫–∞ (—É—è–∑–≤–∏–º–æ—Å—Ç—å –∏–ª–∏ –ø—Ä–æ–±–ª–µ–º–∞)"""
    rule_id: str
    rule_name: Optional[str] = None
    message: Optional[str] = None
    level: SeverityLevel = SeverityLevel.NOTE
    locations: List[Location] = field(default_factory=list)
    
    def __repr__(self):
        loc_count = len(self.locations)
        return f"Finding(rule_id='{self.rule_id}', level={self.level.value}, locations={loc_count})"


@dataclass
class ToolInfo:
    """–ò–Ω—Ñ–æ—Ä–º–∞—Ü–∏—è –æ–± –∏–Ω—Å—Ç—Ä—É–º–µ–Ω—Ç–µ —Å–∫–∞–Ω–∏—Ä–æ–≤–∞–Ω–∏—è"""
    name: str
    version: Optional[str] = None
    organization: Optional[str] = None
    information_uri: Optional[str] = None
    
    def __repr__(self):
        ver = f" v{self.version}" if self.version else ""
        return f"ToolInfo({self.name}{ver})"


@dataclass
class SarifReport:
    """SARIF –æ—Ç—á–µ—Ç"""
    tool: ToolInfo
    rules: Dict[str, Rule] = field(default_factory=dict)
    findings: List[Finding] = field(default_factory=list)
    sarif_version: str = "2.1.0"
    
    def get_statistics(self) -> Dict[str, Any]:
        """–ü–æ–ª—É—á–∏—Ç—å —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫—É –ø–æ –æ—Ç—á–µ—Ç—É"""
        stats = {
            "total_findings": len(self.findings),
            "total_rules": len(self.rules),
            "by_severity": {
                "error": 0,
                "warning": 0,
                "note": 0,
                "none": 0
            },
            "by_rule": {}
        }
        
        for finding in self.findings:
            # –ü–æ–¥—Å—á–µ—Ç –ø–æ —É—Ä–æ–≤–Ω—é —Å–µ—Ä—å–µ–∑–Ω–æ—Å—Ç–∏
            stats["by_severity"][finding.level.value] += 1
            
            # –ü–æ–¥—Å—á–µ—Ç –ø–æ –ø—Ä–∞–≤–∏–ª–∞–º
            rule_id = finding.rule_id
            if rule_id not in stats["by_rule"]:
                stats["by_rule"][rule_id] = {
                    "count": 0,
                    "name": finding.rule_name or rule_id,
                    "level": finding.level.value
                }
            stats["by_rule"][rule_id]["count"] += 1
        
        return stats
    
    def __repr__(self):
        return f"SarifReport(tool={self.tool}, findings={len(self.findings)}, rules={len(self.rules)})"


class SarifParser:
    """–ü–∞—Ä—Å–µ—Ä SARIF –æ—Ç—á–µ—Ç–æ–≤"""
    
    @staticmethod
    def parse_file(file_path: str) -> SarifReport:
        """
        –ü–∞—Ä—Å–∏—Ç—å SARIF —Ñ–∞–π–ª
        
        Args:
            file_path: –ø—É—Ç—å –∫ SARIF —Ñ–∞–π–ª—É
            
        Returns:
            SarifReport –æ–±—ä–µ–∫—Ç
        """
        with open(file_path, 'r', encoding='utf-8') as f:
            data = json.load(f)
        
        return SarifParser.parse_dict(data)
    
    @staticmethod
    def parse_dict(data: Dict[str, Any]) -> SarifReport:
        """
        –ü–∞—Ä—Å–∏—Ç—å SARIF –¥–∞–Ω–Ω—ã–µ –∏–∑ —Å–ª–æ–≤–∞—Ä—è
        
        Args:
            data: —Å–ª–æ–≤–∞—Ä—å —Å SARIF –¥–∞–Ω–Ω—ã–º–∏
            
        Returns:
            SarifReport –æ–±—ä–µ–∫—Ç
        """
        sarif_version = data.get("version", "2.1.0")
        
        # –ü–∞—Ä—Å–∏–º –ø–µ—Ä–≤—ã–π run (–æ–±—ã—á–Ω–æ –æ–Ω –æ–¥–∏–Ω)
        if not data.get("runs"):
            raise ValueError("SARIF —Ñ–∞–π–ª –Ω–µ —Å–æ–¥–µ—Ä–∂–∏—Ç –∑–∞–ø—É—Å–∫–æ–≤ (runs)")
        
        run = data["runs"][0]
        
        # –ü–∞—Ä—Å–∏–º –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é –æ–± –∏–Ω—Å—Ç—Ä—É–º–µ–Ω—Ç–µ
        tool_data = run.get("tool", {}).get("driver", {})
        tool = SarifParser._parse_tool(tool_data)
        
        # –ü–∞—Ä—Å–∏–º –ø—Ä–∞–≤–∏–ª–∞
        rules = SarifParser._parse_rules(tool_data.get("rules", []))
        
        # –ü–∞—Ä—Å–∏–º –Ω–∞—Ö–æ–¥–∫–∏
        findings = SarifParser._parse_findings(run.get("results", []), rules)
        
        return SarifReport(
            tool=tool,
            rules=rules,
            findings=findings,
            sarif_version=sarif_version
        )
    
    @staticmethod
    def _parse_tool(tool_data: Dict[str, Any]) -> ToolInfo:
        """–ü–∞—Ä—Å–∏—Ç—å –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é –æ–± –∏–Ω—Å—Ç—Ä—É–º–µ–Ω—Ç–µ"""
        return ToolInfo(
            name=tool_data.get("name", "Unknown"),
            version=tool_data.get("version"),
            organization=tool_data.get("organization"),
            information_uri=tool_data.get("informationUri")
        )
    
    @staticmethod
    def _parse_rules(rules_data: List[Dict[str, Any]]) -> Dict[str, Rule]:
        """–ü–∞—Ä—Å–∏—Ç—å –ø—Ä–∞–≤–∏–ª–∞"""
        rules = {}
        
        for rule_data in rules_data:
            rule_id = rule_data.get("id", "")
            
            # –ò–∑–≤–ª–µ–∫–∞–µ–º –æ–ø–∏—Å–∞–Ω–∏–µ
            full_desc = rule_data.get("fullDescription", {})
            desc_text = full_desc.get("text")
            desc_markdown = full_desc.get("markdown")
            
            # –ò–∑–≤–ª–µ–∫–∞–µ–º —É—Ä–æ–≤–µ–Ω—å —Å–µ—Ä—å–µ–∑–Ω–æ—Å—Ç–∏
            config = rule_data.get("defaultConfiguration", {})
            level_str = config.get("level", "note")
            try:
                level = SeverityLevel(level_str)
            except ValueError:
                level = SeverityLevel.NOTE
            
            rule = Rule(
                id=rule_id,
                name=rule_data.get("name", rule_id),
                description_text=desc_text,
                description_markdown=desc_markdown,
                level=level,
                enabled=config.get("enabled", True)
            )
            
            rules[rule_id] = rule
        
        return rules
    
    @staticmethod
    def _parse_findings(results_data: List[Dict[str, Any]], 
                       rules: Dict[str, Rule]) -> List[Finding]:
        """–ü–∞—Ä—Å–∏—Ç—å –Ω–∞—Ö–æ–¥–∫–∏"""
        findings = []
        
        for result in results_data:
            rule_id = result.get("ruleId", "")
            
            # –ü–æ–ª—É—á–∞–µ–º –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é –æ –ø—Ä–∞–≤–∏–ª–µ
            rule = rules.get(rule_id)
            rule_name = rule.name if rule else rule_id
            level = rule.level if rule else SeverityLevel.NOTE
            
            # –ò–∑–≤–ª–µ–∫–∞–µ–º —Å–æ–æ–±—â–µ–Ω–∏–µ
            message_data = result.get("message", {})
            message = message_data.get("text")
            
            # –ü–∞—Ä—Å–∏–º –ª–æ–∫–∞—Ü–∏–∏
            locations = SarifParser._parse_locations(result.get("locations", []))
            
            finding = Finding(
                rule_id=rule_id,
                rule_name=rule_name,
                message=message,
                level=level,
                locations=locations
            )
            
            findings.append(finding)
        
        return findings
    
    @staticmethod
    def _parse_locations(locations_data: List[Dict[str, Any]]) -> List[Location]:
        """–ü–∞—Ä—Å–∏—Ç—å –ª–æ–∫–∞—Ü–∏–∏"""
        locations = []
        
        for loc_data in locations_data:
            physical_loc = loc_data.get("physicalLocation", {})
            
            # –ò–∑–≤–ª–µ–∫–∞–µ–º –ø—É—Ç—å –∫ —Ñ–∞–π–ª—É
            artifact_loc = physical_loc.get("artifactLocation", {})
            file_path = artifact_loc.get("uri", "")
            
            # –ò–∑–≤–ª–µ–∫–∞–µ–º —Ä–µ–≥–∏–æ–Ω (—Å—Ç—Ä–æ–∫–∏, –∫–æ–ª–æ–Ω–∫–∏)
            region = physical_loc.get("region", {})
            start_line = region.get("startLine")
            end_line = region.get("endLine")
            start_column = region.get("startColumn")
            end_column = region.get("endColumn")
            
            # –ò–∑–≤–ª–µ–∫–∞–µ–º —Ñ—Ä–∞–≥–º–µ–Ω—Ç –∫–æ–¥–∞
            snippet_data = region.get("snippet", {})
            snippet = snippet_data.get("text")
            
            location = Location(
                file_path=file_path,
                snippet=snippet,
                start_line=start_line,
                end_line=end_line,
                start_column=start_column,
                end_column=end_column
            )
            
            locations.append(location)
        
        return locations


def print_report_summary(report: SarifReport):
    """–í—ã–≤–µ—Å—Ç–∏ –∫—Ä–∞—Ç–∫—É—é –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é –æ –æ—Ç—á–µ—Ç–µ"""
    print(f"\n{'='*80}")
    print(f"SARIF –û—Ç—á–µ—Ç: {report.tool.name}")
    if report.tool.version:
        print(f"–í–µ—Ä—Å–∏—è: {report.tool.version}")
    if report.tool.organization:
        print(f"–û—Ä–≥–∞–Ω–∏–∑–∞—Ü–∏—è: {report.tool.organization}")
    print(f"{'='*80}\n")
    
    stats = report.get_statistics()
    
    print(f"–í—Å–µ–≥–æ –Ω–∞—Ö–æ–¥–æ–∫: {stats['total_findings']}")
    print(f"–í—Å–µ–≥–æ –ø—Ä–∞–≤–∏–ª: {stats['total_rules']}\n")
    
    print("–†–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ –ø–æ —É—Ä–æ–≤–Ω—è–º —Å–µ—Ä—å–µ–∑–Ω–æ—Å—Ç–∏:")
    print(f"  üî¥ –û—à–∏–±–∫–∏ (error):    {stats['by_severity']['error']}")
    print(f"  üü° –ü—Ä–µ–¥—É–ø—Ä–µ–∂–¥–µ–Ω–∏—è (warning): {stats['by_severity']['warning']}")
    print(f"  üîµ –ó–∞–º–µ—Ç–∫–∏ (note):     {stats['by_severity']['note']}")
    print(f"  ‚ö™ –ü—Ä–æ—á–∏–µ (none):      {stats['by_severity']['none']}\n")
    
    print("–¢–æ–ø-10 –ø—Ä–∞–≤–∏–ª –ø–æ –∫–æ–ª–∏—á–µ—Å—Ç–≤—É –Ω–∞—Ö–æ–¥–æ–∫:")
    sorted_rules = sorted(
        stats['by_rule'].items(), 
        key=lambda x: x[1]['count'], 
        reverse=True
    )
    
    for i, (rule_id, rule_info) in enumerate(sorted_rules[:10], 1):
        level_icon = {
            'error': 'üî¥',
            'warning': 'üü°',
            'note': 'üîµ',
            'none': '‚ö™'
        }.get(rule_info['level'], '‚ö™')
        
        print(f"  {i:2}. {level_icon} [{rule_info['count']:4}] {rule_info['name']}")
        print(f"      ID: {rule_id}")
    
    print(f"\n{'='*80}\n")


def print_detailed_findings(report: SarifReport, max_findings: int = 10):
    """–í—ã–≤–µ—Å—Ç–∏ –¥–µ—Ç–∞–ª—å–Ω—É—é –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é –æ –Ω–∞—Ö–æ–¥–∫–∞—Ö"""
    print(f"–î–µ—Ç–∞–ª—å–Ω—ã–π —Å–ø–∏—Å–æ–∫ –Ω–∞—Ö–æ–¥–æ–∫ (–ø–µ—Ä–≤—ã–µ {max_findings}):\n")
    
    for i, finding in enumerate(report.findings[:max_findings], 1):
        level_icon = {
            SeverityLevel.ERROR: 'üî¥',
            SeverityLevel.WARNING: 'üü°',
            SeverityLevel.NOTE: 'üîµ',
            SeverityLevel.NONE: '‚ö™'
        }.get(finding.level, '‚ö™')
        
        print(f"{i}. {level_icon} {finding.rule_name}")
        print(f"   ID: {finding.rule_id}")
        print(f"   –£—Ä–æ–≤–µ–Ω—å: {finding.level.value}")
        
        if finding.message:
            print(f"   –°–æ–æ–±—â–µ–Ω–∏–µ: {finding.message}")
        
        if finding.locations:
            print(f"   –õ–æ–∫–∞—Ü–∏–∏ ({len(finding.locations)}):")
            for loc in finding.locations[:3]:  # –ü–æ–∫–∞–∑—ã–≤–∞–µ–º —Ç–æ–ª—å–∫–æ –ø–µ—Ä–≤—ã–µ 3 –ª–æ–∫–∞—Ü–∏–∏
                print(f"     - {loc.file_path}")
                if loc.start_line:
                    print(f"       –°—Ç—Ä–æ–∫–∞: {loc.start_line}")
                if loc.snippet:
                    # –ü–æ–∫–∞–∑—ã–≤–∞–µ–º —Ç–æ–ª—å–∫–æ –ø–µ—Ä–≤—ã–µ 100 —Å–∏–º–≤–æ–ª–æ–≤ —Å–Ω–∏–ø–ø–µ—Ç–∞
                    snippet_preview = loc.snippet[:100].replace('\n', ' ')
                    if len(loc.snippet) > 100:
                        snippet_preview += "..."
                    print(f"       –ö–æ–¥: {snippet_preview}")
            
            if len(finding.locations) > 3:
                print(f"     ... –∏ –µ—â–µ {len(finding.locations) - 3} –ª–æ–∫–∞—Ü–∏–π")
        
        print()


if __name__ == "__main__":
    # –ü—Ä–∏–º–µ—Ä –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—è
    import sys
    
    if len(sys.argv) < 2:
        print("–ò—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ: python sarif_parser.py <–ø—É—Ç—å_–∫_sarif_—Ñ–∞–π–ª—É>")
        print("\n–ü—Ä–∏–º–µ—Ä:")
        print("  python sarif_parser.py report.sarif")
        sys.exit(1)
    
    sarif_file = sys.argv[1]
    
    try:
        print(f"–ü–∞—Ä—Å–∏–Ω–≥ —Ñ–∞–π–ª–∞: {sarif_file}")
        report = SarifParser.parse_file(sarif_file)
        
        print_report_summary(report)
        print_detailed_findings(report)
        
    except FileNotFoundError:
        print(f"‚ùå –û—à–∏–±–∫–∞: –§–∞–π–ª '{sarif_file}' –Ω–µ –Ω–∞–π–¥–µ–Ω")
        sys.exit(1)
    except json.JSONDecodeError as e:
        print(f"‚ùå –û—à–∏–±–∫–∞ –ø–∞—Ä—Å–∏–Ω–≥–∞ JSON: {e}")
        sys.exit(1)
    except Exception as e:
        print(f"‚ùå –û—à–∏–±–∫–∞: {e}")
        import traceback
        traceback.print_exc()
        sys.exit(1)

